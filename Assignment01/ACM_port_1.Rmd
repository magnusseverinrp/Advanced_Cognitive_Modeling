---
title: "ACM portfolio 1"
author: "Magnus Severin Ringgaard Poulsen"
date: "2026-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup 
```{r}
pacman::p_load(tidyverse, patchwork)
```

```{r}
# Define trials and agents
trials <- 100
agents <- 100  
```


## Win-Stay-Lose-Shift Agent 
```{r}
# Define Agent Function 
WSLS_agent_f <- function(prev_round_choice, feedback) { #agent parameters 
  
  choice <- ifelse(feedback == 1, prev_round_choice, 1 - prev_round_choice) #pick the opposite of last of previous choice (1-prev_round_choice) if lost, pick the same if won. 

  return(choice) #make choice 
}
```

## Reinforcement Learning Agent
```{r}
rl_agent_f <- function(last_choice, estimated_value, feedback, alpha = 0.4, beta=1) { 
  k <- last_choice + 1 
  estimated_value[k] <- estimated_value[k] + alpha * (feedback - estimated_value[k])
  
  #softmaxxing
  exp_p <- exp(beta * estimated_value)
  p <- exp_p / sum(exp_p)
  
  choice <- sample(c(0, 1), 1, prob = p)
  
   return(list(
    choice = choice,
    estimated_value = estimated_value
  ))
}
```

## Simulating - WSLS vs. RL
```{r}
full_simulation <- function(alpha) {

# Create empty lists for choices and feedback
wsls_choices <- rep(NA, trials)
RL_choices <- rep(NA, trials)
feedback_wsls <- rep(NA, trials)
feedback_RL <- rep(NA, trials)
estimated_value_tracked <- matrix(NA, trials, 2)
estimated_value_tracked[1, ] <- c(0.5, 0.5)

wsls_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 1 
RL_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 2 

for (t in 2:trials) {
  # Feedback for agent 1 from previous trial (wins if match)
  prev_feedback1 <- ifelse(wsls_choices[t - 1] == RL_choices[t - 1], 1, 0)
  feedback_wsls[t - 1] <- prev_feedback1

  # Feedback for agent 2 (opponent) is the opposite (wins if mismatch)
  prev_feedback2 <- 1 - prev_feedback1
  feedback_RL[t - 1] <- prev_feedback2

  # Both agents choose based on their own previous feedback
  wsls_choices[t] <- WSLS_agent_f(wsls_choices[t - 1], prev_feedback1) #WSLS agent choice
  
  result <- rl_agent_f(
    RL_choices[t - 1],
    estimated_value_tracked[t - 1, ],
    prev_feedback2,
    alpha=alpha
  )
  
  RL_choices[t] <- result$choice
  estimated_value_tracked[t, ] <- result$estimated_value
  
  
}
feedback_wsls[trials] <- ifelse(wsls_choices[trials] == RL_choices[trials], 1, 0)

# Create dataframe
df_WSLS_RL <- tibble(
  trial = 1:trials,
  Self_WSLS = wsls_choices,
  Opponent_RL = RL_choices,
  Feedback_wsls = feedback_wsls, # 1 if Self_WSLS won
  Feedback_RL = feedback_RL, # 1 if Opponent_RL won
  RL_estimated_value = estimated_value_tracked,
  alpha=alpha
) %>% mutate(
  Cumulative_Performance = cumsum(feedback_wsls) / row_number()
)

return(df_WSLS_RL)
}
```

## Run simulation (differing alpha)
```{r}
alpha_values <- seq(from = 0, to = 1, by = 0.1)

df <- map_dfr(alpha_values, full_simulation) #running full simulation for all alpha values appending each df_WSLS_RL to create df containing simulations for each alpha value
```


## Visualization - WSLS vs. RL (alpha=0.4)
```{r}
# Visualize Simulation Results 

p_choices_wsls <- ggplot(df %>% filter(alpha == 0.4), aes(x = trial)) +
  geom_line(aes(y = Self_WSLS, color = "WSLS Agent")) +
  geom_line(aes(y = Opponent_RL + 0.05, color = "RL Agent"), linetype = "dashed") + # Offset slightly
  labs(title = "WSLS vs. RL (alpha=0.4)", y = "Choice (0/1)") + theme_classic() + ylim(-0.1, 1.1)

p_perf_wsls <- ggplot(df %>% filter(alpha == 0.4), aes(x = trial, y = Cumulative_Performance)) +
  geom_line(color = "purple", size = 1) + geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(title = "WSLS Performance vs. RL (alpha=0.4)", y = "Proportion Wins") + theme_classic() + ylim(0, 1)

ggsave(p_choices_wsls, filename= "choices_alpha_0.4.jpg")

ggsave(p_perf_wsls, filename= "cumulative_alpha_0.4.jpg")
```


## Visualization - Estimated Value (alpha = 0.4)
```{r}
# Visualize Simulation Results 

p_choices_ev <- ggplot(df %>% filter(alpha == 0.4), aes(x = trial)) +
  geom_line(aes(y = RL_estimated_value[,1], color = "RL Agent (left hand)"), linetype = "solid") +
  geom_line(aes(y = RL_estimated_value[,2], color = "RL Agent (right hand)"), linetype = "solid") +
  labs(title = "Reinforcement Learning - Estimated Value (alpha=0.4)", y = "Estimated Value") + theme_classic() + ylim(-0.1, 1.1)

ggsave(p_choices_ev, filename= "ev_alpha_0.4.jpg")
```
## Visualization - WSLS vs. RL at differing alpha levels
```{r}
# Visualize Simulation Results 

p_choices_wsls <- ggplot(df, aes(x = trial)) +
  facet_wrap(~alpha) + 
  geom_line(aes(y = Self_WSLS, color = "WSLS Agent")) +
  geom_line(aes(y = Opponent_RL + 0.05, color = "RL Agent"), linetype = "dashed") + # Offset slightly
  labs(title = "WSLS vs. RL by alpha level", y = "Choice (0/1)") + theme_classic() + ylim(-0.1, 1.1)

p_perf_wsls <- ggplot(df, aes(x = trial, y = Cumulative_Performance)) +
  facet_wrap(~alpha) + 
  geom_line(color = "purple", size = 1) + geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(title = "WSLS Performance vs. RL by alpha level", y = "Proportion Wins") + theme_classic() + ylim(0, 1)

ggsave(p_choices_wsls, filename= "choices_alpha_facet_wrapped.jpg")  

ggsave(p_perf_wsls, filename= "cumulative_alpha_facet_wrapped.jpg")  
```

## Visualization - Estimated Value at differing alpha levels
```{r}
# Visualize Simulation Results 

p_choices_ev <- ggplot(df, aes(x = trial)) +
  facet_wrap(~alpha) +
  geom_line(aes(y = RL_estimated_value[,1], color = "RL Agent (left hand)"), linetype = "solid") +
  geom_line(aes(y = RL_estimated_value[,2], color = "RL Agent (right hand)"), linetype = "solid") +
  labs(title = "Reinforcement Learning - Estimated Value by alpha level", y = "Estimated Value") + theme_classic() + ylim(-0.1, 1.1)

ggsave(p_choices_ev, filename= "ev_alpha_facet_wrapped.jpg")  
```

