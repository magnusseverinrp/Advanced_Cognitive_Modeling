---
title: "ACM portfolio 1"
author: "Magnus Severin Ringgaard Poulsen"
date: "2026-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup 
```{r}
pacman::p_load(tidyverse, patchwork)
```

```{r}
# Define trials and agents
trials <- 250  

agents <- 100  
```


## Win-Stay-Lose-Shirt Agent 
```{r}
# Define Agent Function 
WSLS_agent_f <- function(prev_round_choice, feedback) { #agent parameters 
  
  choice <- ifelse(feedback == 1, prev_round_choice, 1 - prev_round_choice) #pick the opposite of last of                                     previous choice (1-prev_round_choice) if lost, pick the same if won. 

  return(choice) #make choice 
}
```

## Reinforcement Learning Agent
```{r}
rl_agent_f <- function(last_choice, estimated_value, feedback, alpha = 1, beta=2) { 
  k <- last_choice + 1 
  estimated_value[k] <- estimated_value[k] + alpha * (feedback - estimated_value[k])
  
  #softmaxxing
  exp_p <- exp(beta * estimated_value)
  p <- exp_p / sum(exp_p)
  
  choice <- sample(c(0, 1), 1, prob = p)
  
  #choice <- rbinom(1, 1, estimated_value) #old way of choosing delete
  
   return(list(
    choice = choice,
    estimated_value = estimated_value
  ))
}
```


## Simulating - WSLS vs. WSLS 
```{r}
# Create empty lists for choices and feedback
wsls1_choices <- rep(NA, trials)
wsls2_choices <- rep(NA, trials)
feedback_wsls1 <- rep(NA, trials)

wsls1_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 1 
wsls2_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 2 

for (t in 2:trials) {
  # Feedback for agent 1 from previous trial (wins if match)
  prev_feedback1 <- ifelse(wsls1_choices[t - 1] == wsls2_choices[t - 1], 1, 0)
  feedback_wsls1[t - 1] <- prev_feedback1

  # Feedback for agent 2 (opponent) is the opposite (wins if mismatch)
  prev_feedback2 <- 1 - prev_feedback1

  # Both agents choose based on their own previous feedback
  wsls1_choices[t] <- WSLS_agent_f(wsls1_choices[t - 1], prev_feedback1)
  wsls2_choices[t] <- WSLS_agent_f(wsls2_choices[t - 1], prev_feedback2)
}
feedback_wsls1[trials] <- ifelse(wsls1_choices[trials] == wsls2_choices[trials], 1, 0)

# Create dataframe
df_vs_wsls <- tibble(
  trial = 1:trials,
  Self_WSLS = wsls1_choices,
  Opponent_WSLS = wsls2_choices,
  Feedback = feedback_wsls1 # 1 if Self_WSLS won
) %>% mutate(
  Cumulative_Performance = cumsum(Feedback) / row_number()
)


```

## Visualization - WSLS vs. WSLS 
```{r}
# Visualize Simulation Results 

p_choices_wsls <- ggplot(df_vs_wsls, aes(x = trial)) +
  geom_line(aes(y = Self_WSLS, color = "WSLS Agent 1")) +
  geom_line(aes(y = Opponent_WSLS + 0.05, color = "WSLS Agent 2"), linetype = "dashed") + # Offset slightly
  labs(title = "WSLS vs. WSLS", y = "Choice (0/1)") + theme_classic() + ylim(-0.1, 1.1)

p_perf_wsls <- ggplot(df_vs_wsls, aes(x = trial, y = Cumulative_Performance)) +
  geom_line(color = "purple", size = 1) + geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(title = "WSLS Performance vs. WSLS", y = "Proportion Wins") + theme_classic() + ylim(0, 1)

p_choices_wsls

p_perf_wsls
```

## Simulating - WSLS vs. RL
```{r}
# Create empty lists for choices and feedback
wsls_choices <- rep(NA, trials)
RL_choices <- rep(NA, trials)
feedback_wsls <- rep(NA, trials)
feedback_RL <- rep(NA, trials)
estimated_value_tracked <- matrix(NA, trials, 2)
estimated_value_tracked[1, ] <- c(0.5, 0.5)

wsls_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 1 
RL_choices[1] <- sample(c(0, 1), 1) #choice on trial 1 for agent 2 
estimated_value_tracked[1] <- 0.5

for (t in 2:trials) {
  # Feedback for agent 1 from previous trial (wins if match)
  prev_feedback1 <- ifelse(wsls_choices[t - 1] == RL_choices[t - 1], 1, 0)
  feedback_wsls[t - 1] <- prev_feedback1

  # Feedback for agent 2 (opponent) is the opposite (wins if mismatch)
  prev_feedback2 <- 1 - prev_feedback1
  feedback_RL[t - 1] <- prev_feedback2

  # Both agents choose based on their own previous feedback
  wsls_choices[t] <- WSLS_agent_f(wsls_choices[t - 1], prev_feedback1) #WSLS agent choice
  
  result <- rl_agent_f(
    RL_choices[t - 1],
    estimated_value_tracked[t - 1, ],
    prev_feedback2
  )
  
  RL_choices[t] <- result$choice
  estimated_value_tracked[t, ] <- result$estimated_value
  
  
}
feedback_wsls[trials] <- ifelse(wsls_choices[trials] == RL_choices[trials], 1, 0)

# Create dataframe
df_WSLS_RL <- tibble(
  trial = 1:trials,
  Self_WSLS = wsls_choices,
  Opponent_RL = RL_choices,
  Feedback_wsls = feedback_wsls, # 1 if Self_WSLS won
  Feedback_RL = feedback_RL, # 1 if Opponent_RL won
  RL_estimated_value = estimated_value_tracked
) %>% mutate(
  Cumulative_Performance = cumsum(feedback_wsls) / row_number()
)

```

## Visualization - WSLS vs. RL 
```{r}
# Visualize Simulation Results 

p_choices_wsls <- ggplot(df_WSLS_RL, aes(x = trial)) +
  geom_line(aes(y = Self_WSLS, color = "WSLS Agent")) +
  geom_line(aes(y = Opponent_RL + 0.05, color = "RL Agent"), linetype = "dashed") + # Offset slightly
  labs(title = "WSLS vs. RL", y = "Choice (0/1)") + theme_classic() + ylim(-0.1, 1.1)

p_perf_wsls <- ggplot(df_WSLS_RL, aes(x = trial, y = Cumulative_Performance)) +
  geom_line(color = "purple", size = 1) + geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(title = "WSLS Performance vs. RL", y = "Proportion Wins") + theme_classic() + ylim(0, 1)

p_choices_wsls

p_perf_wsls
```


## Visualization - Estimated Value 
```{r}
# Visualize Simulation Results 

p_choices_wsls <- ggplot(df_WSLS_RL, aes(x = trial)) +
  geom_line(aes(y = Opponent_RL, color = "RL Agent"), linetype = "dashed") + # Offset slightly
  labs(title = "WSLS vs. RL", y = "Choice (0/1)") + theme_classic() + ylim(-0.1, 1.1)

p_perf_wsls <- ggplot(df_WSLS_RL, aes(x = trial, y = Cumulative_Performance)) +
  geom_line(color = "purple", size = 1) + geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(title = "WSLS Performance vs. RL", y = "Proportion Wins") + theme_classic() + ylim(0, 1)

p_choices_wsls

p_perf_wsls
```


